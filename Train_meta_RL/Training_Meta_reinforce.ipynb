{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of experiment-with-RBF.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDWM6OGTnzx3"
      },
      "source": [
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.ppo.policies import MlpPolicy\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
        "from stable_baselines3.common.noise import NormalActionNoise\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "import torch as th"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STfDRbqWoCQW"
      },
      "source": [
        "from stable_baselines3.ppo.policies import MlpPolicy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxBA1yuNow4F"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "from environment import *\n",
        "import tensorflow as tf\n",
        "import tqdm\n",
        "import pandas as pd\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZlyldO8o1ql"
      },
      "source": [
        "data1 = np.array(pd.read_csv(\"dataset1\"))\n",
        "data2 = np.array(pd.read_csv(\"dataset2\"))\n",
        "data3 = np.array(pd.read_csv(\"dataset3\"))\n",
        "data4 = np.array(pd.read_csv(\"dataset4\"))\n",
        "data5 = np.array(pd.read_csv(\"dataset5\"))\n",
        "data6 = np.array(pd.read_csv(\"dataset6\"))\n",
        "data7 = np.array(pd.read_csv(\"dataset7\"))\n",
        "data8 = np.array(pd.read_csv(\"dataset8\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dACSmJ6LRhjq"
      },
      "source": [
        "from stable_baselines3.common.env_checker import check_env"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWhzhohSRLNi",
        "outputId": "9f216781-3f71-4064-8c5e-c14ecac0d021"
      },
      "source": [
        "env = train_env(np.array([data1,data2,data3,data4,data5,data6,data7,data8]),max_steps=50000,punish = -15,reward_correct = 1,punish_other = -10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hi\n",
            "50000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbLgK7JOzGBR"
      },
      "source": [
        "check_env(env, warn=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1J25cB2HZR-H"
      },
      "source": [
        "\n",
        "def moving_average(values, window):\n",
        "    \"\"\"\n",
        "    Smooth values by doing a moving average\n",
        "    :param values: (numpy array)\n",
        "    :param window: (int)\n",
        "    :return: (numpy array)\n",
        "    \"\"\"\n",
        "    weights = np.repeat(1.0, window) / window\n",
        "    return np.convolve(values, weights, 'valid')\n",
        "\n",
        "\n",
        "def plot_results(log_folder, title='Learning Curve'):\n",
        "    \"\"\"\n",
        "    plot the results\n",
        "\n",
        "    :param log_folder: (str) the save location of the results to plot\n",
        "    :param title: (str) the title of the task to plot\n",
        "    \"\"\"\n",
        "    x, y = ts2xy(load_results(log_folder), 'timesteps')\n",
        "    y = moving_average(y, window=50)\n",
        "    # Truncate x\n",
        "    x = x[len(x) - len(y):]\n",
        "\n",
        "    fig = plt.figure(title)\n",
        "    plt.plot(x, y)\n",
        "    plt.xlabel('Number of Timesteps')\n",
        "    plt.ylabel('Rewards')\n",
        "    plt.title(title + \" Smoothed\")\n",
        "    plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mx7nFYoxZV9i"
      },
      "source": [
        "import os\n",
        "log_dir = \"/log/RBF\"\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "env = Monitor(env, log_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLnDVKPERdQ2",
        "outputId": "5b866a03-2402-43d4-c9bb-0603714b0397"
      },
      "source": [
        "model =   PPO('MlpPolicy', env,verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cpu device\n",
            "Wrapping the env in a DummyVecEnv.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgVjqXaBozuF",
        "outputId": "ae132fe4-2d1d-4e75-fc48-a17a0dfcbd29"
      },
      "source": [
        "callback = SaveOnBestTrainingRewardCallback(check_freq=50000, log_dir=log_dir)\n",
        "model.learn(total_timesteps=9000000, log_interval=200, callback=callback)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num timesteps: 50000\n",
            "Best mean reward: -inf - Last mean reward per episode: 8568.52\n",
            "Saving new best model to /log/IOE_combine4/RBF/best_model.zip\n",
            "Num timesteps: 100000\n",
            "Best mean reward: 8568.52 - Last mean reward per episode: 8625.39\n",
            "Saving new best model to /log/IOE_combine4/RBF/best_model.zip\n",
            "Num timesteps: 150000\n",
            "Best mean reward: 8625.39 - Last mean reward per episode: 8630.59\n",
            "Saving new best model to /log/IOE_combine4/RBF/best_model.zip\n",
            "Num timesteps: 200000\n",
            "Best mean reward: 8630.59 - Last mean reward per episode: 8659.30\n",
            "Saving new best model to /log/IOE_combine4/RBF/best_model.zip\n",
            "Num timesteps: 250000\n",
            "Best mean reward: 8659.30 - Last mean reward per episode: 8708.97\n",
            "Saving new best model to /log/IOE_combine4/RBF/best_model.zip\n",
            "Num timesteps: 300000\n",
            "Best mean reward: 8708.97 - Last mean reward per episode: 8722.50\n",
            "Saving new best model to /log/IOE_combine4/RBF/best_model.zip\n",
            "Num timesteps: 350000\n",
            "Best mean reward: 8722.50 - Last mean reward per episode: 8765.33\n",
            "Saving new best model to /log/IOE_combine4/RBF/best_model.zip\n",
            "Num timesteps: 400000\n",
            "Best mean reward: 8765.33 - Last mean reward per episode: 8770.41\n",
            "Saving new best model to /log/IOE_combine4/RBF/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 5e+04        |\n",
            "|    ep_rew_mean          | 9.44e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 754          |\n",
            "|    iterations           | 200          |\n",
            "|    time_elapsed         | 542          |\n",
            "|    total_timesteps      | 409600       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0033925234 |\n",
            "|    clip_fraction        | 0.0238       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.208       |\n",
            "|    explained_variance   | -6.67        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 8.27         |\n",
            "|    n_updates            | 8570         |\n",
            "|    policy_gradient_loss | -0.00245     |\n",
            "|    value_loss           | 14.9         |\n",
            "------------------------------------------\n",
            "Num timesteps: 450000\n",
            "Best mean reward: 8770.41 - Last mean reward per episode: 8795.60\n",
            "Saving new best model to /log/IOE_combine4/RBF/best_model.zip\n",
            "Num timesteps: 500000\n",
            "Best mean reward: 8795.60 - Last mean reward per episode: 8819.00\n",
            "Saving new best model to /log/IOE_combine4/RBF/best_model.zip\n",
            "Num timesteps: 550000\n",
            "Best mean reward: 8819.00 - Last mean reward per episode: 8872.68\n",
            "Saving new best model to /log/IOE_combine4/RBF/best_model.zip\n",
            "Num timesteps: 600000\n",
            "Best mean reward: 8872.68 - Last mean reward per episode: 8918.76\n",
            "Saving new best model to /log/IOE_combine4/RBF/best_model.zip\n",
            "Num timesteps: 650000\n",
            "Best mean reward: 8918.76 - Last mean reward per episode: 8986.79\n",
            "Saving new best model to /log/IOE_combine4/RBF/best_model.zip\n",
            "Num timesteps: 700000\n",
            "Best mean reward: 8986.79 - Last mean reward per episode: 9017.55\n",
            "Saving new best model to /log/IOE_combine4/RBF/best_model.zip\n",
            "Num timesteps: 750000\n",
            "Best mean reward: 9017.55 - Last mean reward per episode: 9060.73\n",
            "Saving new best model to /log/IOE_combine4/RBF/best_model.zip\n",
            "Num timesteps: 800000\n",
            "Best mean reward: 9060.73 - Last mean reward per episode: 9092.48\n",
            "Saving new best model to /log/IOE_combine4/RBF/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 5e+04        |\n",
            "|    ep_rew_mean          | 9.95e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 754          |\n",
            "|    iterations           | 400          |\n",
            "|    time_elapsed         | 1085         |\n",
            "|    total_timesteps      | 819200       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0054717045 |\n",
            "|    clip_fraction        | 0.037        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.21        |\n",
            "|    explained_variance   | -62          |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 13.4         |\n",
            "|    n_updates            | 10570        |\n",
            "|    policy_gradient_loss | 0.000787     |\n",
            "|    value_loss           | 28.8         |\n",
            "------------------------------------------\n",
            "Num timesteps: 850000\n",
            "Best mean reward: 9092.48 - Last mean reward per episode: 9124.21\n",
            "Saving new best model to /log/IOE_combine4/RBF/best_model.zip\n",
            "Num timesteps: 900000\n",
            "Best mean reward: 9124.21 - Last mean reward per episode: 9160.95\n",
            "Saving new best model to /log/IOE_combine4/RBF/best_model.zip\n",
            "Num timesteps: 950000\n",
            "Best mean reward: 9160.95 - Last mean reward per episode: 9190.80\n",
            "Saving new best model to /log/IOE_combine4/RBF/best_model.zip\n",
            "Num timesteps: 1000000\n",
            "Best mean reward: 9190.80 - Last mean reward per episode: 9229.89\n",
            "Saving new best model to /log/IOE_combine4/RBF/best_model.zip\n",
            "Num timesteps: 1050000\n",
            "Best mean reward: 9229.89 - Last mean reward per episode: 9242.81\n",
            "Saving new best model to /log/IOE_combine4/RBF/best_model.zip\n",
            "Num timesteps: 1100000\n",
            "Best mean reward: 9242.81 - Last mean reward per episode: 9256.48\n",
            "Saving new best model to /log/IOE_combine4/RBF/best_model.zip\n",
            "Num timesteps: 1150000\n",
            "Best mean reward: 9256.48 - Last mean reward per episode: 9273.80\n",
            "Saving new best model to /log/IOE_combine4/RBF/best_model.zip\n",
            "Num timesteps: 1200000\n",
            "Best mean reward: 9273.80 - Last mean reward per episode: 9285.98\n",
            "Saving new best model to /log/IOE_combine4/RBF/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 5e+04        |\n",
            "|    ep_rew_mean          | 1.01e+04     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 754          |\n",
            "|    iterations           | 600          |\n",
            "|    time_elapsed         | 1627         |\n",
            "|    total_timesteps      | 1228800      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0027700986 |\n",
            "|    clip_fraction        | 0.0499       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.314       |\n",
            "|    explained_variance   | -4.33        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 10.5         |\n",
            "|    n_updates            | 12570        |\n",
            "|    policy_gradient_loss | -0.0055      |\n",
            "|    value_loss           | 24           |\n",
            "------------------------------------------\n",
            "Num timesteps: 1250000\n",
            "Best mean reward: 9285.98 - Last mean reward per episode: 9325.43\n",
            "Saving new best model to /log/IOE_combine4/RBF/best_model.zip\n",
            "Num timesteps: 1300000\n",
            "Best mean reward: 9325.43 - Last mean reward per episode: 9354.06\n",
            "Saving new best model to /log/IOE_combine4/RBF/best_model.zip\n",
            "Num timesteps: 1350000\n",
            "Best mean reward: 9354.06 - Last mean reward per episode: 9351.55\n",
            "Num timesteps: 1400000\n",
            "Best mean reward: 9354.06 - Last mean reward per episode: 9360.43\n",
            "Saving new best model to /log/IOE_combine4/RBF/best_model.zip\n",
            "Num timesteps: 1450000\n",
            "Best mean reward: 9360.43 - Last mean reward per episode: 9300.60\n",
            "Num timesteps: 1500000\n",
            "Best mean reward: 9360.43 - Last mean reward per episode: 9272.70\n",
            "Num timesteps: 1550000\n",
            "Best mean reward: 9360.43 - Last mean reward per episode: 9293.18\n",
            "Num timesteps: 1600000\n",
            "Best mean reward: 9360.43 - Last mean reward per episode: 9227.98\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 5e+04       |\n",
            "|    ep_rew_mean          | 9.77e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 754         |\n",
            "|    iterations           | 800         |\n",
            "|    time_elapsed         | 2170        |\n",
            "|    total_timesteps      | 1638400     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002900936 |\n",
            "|    clip_fraction        | 0.039       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.258      |\n",
            "|    explained_variance   | -16.9       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 3.49        |\n",
            "|    n_updates            | 14570       |\n",
            "|    policy_gradient_loss | -0.00172    |\n",
            "|    value_loss           | 6.13        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 1650000\n",
            "Best mean reward: 9360.43 - Last mean reward per episode: 9220.97\n",
            "Num timesteps: 1700000\n",
            "Best mean reward: 9360.43 - Last mean reward per episode: 9224.55\n",
            "Num timesteps: 1750000\n",
            "Best mean reward: 9360.43 - Last mean reward per episode: 9239.20\n",
            "Num timesteps: 1800000\n",
            "Best mean reward: 9360.43 - Last mean reward per episode: 9263.39\n",
            "Num timesteps: 1850000\n",
            "Best mean reward: 9360.43 - Last mean reward per episode: 9297.35\n",
            "Num timesteps: 1900000\n",
            "Best mean reward: 9360.43 - Last mean reward per episode: 9316.17\n",
            "Num timesteps: 1950000\n",
            "Best mean reward: 9360.43 - Last mean reward per episode: 9342.54\n",
            "Num timesteps: 2000000\n",
            "Best mean reward: 9360.43 - Last mean reward per episode: 9354.03\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 5e+04        |\n",
            "|    ep_rew_mean          | 9.87e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 751          |\n",
            "|    iterations           | 1000         |\n",
            "|    time_elapsed         | 2723         |\n",
            "|    total_timesteps      | 2048000      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0034430623 |\n",
            "|    clip_fraction        | 0.0785       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.162       |\n",
            "|    explained_variance   | -6.42        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.724        |\n",
            "|    n_updates            | 16570        |\n",
            "|    policy_gradient_loss | -0.00314     |\n",
            "|    value_loss           | 2.19         |\n",
            "------------------------------------------\n",
            "Num timesteps: 2050000\n",
            "Best mean reward: 9360.43 - Last mean reward per episode: 9370.88\n",
            "Saving new best model to /log/IOE_combine4/RBF/best_model.zip\n",
            "Num timesteps: 2100000\n",
            "Best mean reward: 9370.88 - Last mean reward per episode: 9382.62\n",
            "Saving new best model to /log/IOE_combine4/RBF/best_model.zip\n",
            "Num timesteps: 2150000\n",
            "Best mean reward: 9382.62 - Last mean reward per episode: 9402.99\n",
            "Saving new best model to /log/IOE_combine4/RBF/best_model.zip\n",
            "Num timesteps: 2200000\n",
            "Best mean reward: 9402.99 - Last mean reward per episode: 9402.43\n",
            "Num timesteps: 2250000\n",
            "Best mean reward: 9402.99 - Last mean reward per episode: 9376.15\n",
            "Num timesteps: 2300000\n",
            "Best mean reward: 9402.99 - Last mean reward per episode: 9330.79\n",
            "Num timesteps: 2350000\n",
            "Best mean reward: 9402.99 - Last mean reward per episode: 9332.63\n",
            "Num timesteps: 2400000\n",
            "Best mean reward: 9402.99 - Last mean reward per episode: 9325.03\n",
            "Num timesteps: 2450000\n",
            "Best mean reward: 9402.99 - Last mean reward per episode: 9340.92\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 5e+04       |\n",
            "|    ep_rew_mean          | 9.75e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 749         |\n",
            "|    iterations           | 1200        |\n",
            "|    time_elapsed         | 3279        |\n",
            "|    total_timesteps      | 2457600     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007745699 |\n",
            "|    clip_fraction        | 0.0679      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.52       |\n",
            "|    explained_variance   | -30.6       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 18          |\n",
            "|    n_updates            | 18570       |\n",
            "|    policy_gradient_loss | -0.00327    |\n",
            "|    value_loss           | 33.9        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 2500000\n",
            "Best mean reward: 9402.99 - Last mean reward per episode: 9352.00\n",
            "Num timesteps: 2550000\n",
            "Best mean reward: 9402.99 - Last mean reward per episode: 9368.35\n",
            "Num timesteps: 2600000\n",
            "Best mean reward: 9402.99 - Last mean reward per episode: 9354.23\n",
            "Num timesteps: 2650000\n",
            "Best mean reward: 9402.99 - Last mean reward per episode: 9363.94\n",
            "Num timesteps: 2700000\n",
            "Best mean reward: 9402.99 - Last mean reward per episode: 9370.27\n",
            "Num timesteps: 2750000\n",
            "Best mean reward: 9402.99 - Last mean reward per episode: 9394.77\n",
            "Num timesteps: 2800000\n",
            "Best mean reward: 9402.99 - Last mean reward per episode: 9417.24\n",
            "Saving new best model to /log/IOE_combine4/RBF/best_model.zip\n",
            "Num timesteps: 2850000\n",
            "Best mean reward: 9417.24 - Last mean reward per episode: 9418.82\n",
            "Saving new best model to /log/IOE_combine4/RBF/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 5e+04        |\n",
            "|    ep_rew_mean          | 9.81e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 749          |\n",
            "|    iterations           | 1400         |\n",
            "|    time_elapsed         | 3827         |\n",
            "|    total_timesteps      | 2867200      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0073229154 |\n",
            "|    clip_fraction        | 0.034        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.266       |\n",
            "|    explained_variance   | -15.7        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 15.2         |\n",
            "|    n_updates            | 20570        |\n",
            "|    policy_gradient_loss | -0.00199     |\n",
            "|    value_loss           | 45.7         |\n",
            "------------------------------------------\n",
            "Num timesteps: 2900000\n",
            "Best mean reward: 9418.82 - Last mean reward per episode: 9405.30\n",
            "Num timesteps: 2950000\n",
            "Best mean reward: 9418.82 - Last mean reward per episode: 9358.35\n",
            "Num timesteps: 3000000\n",
            "Best mean reward: 9418.82 - Last mean reward per episode: 9351.58\n",
            "Num timesteps: 3050000\n",
            "Best mean reward: 9418.82 - Last mean reward per episode: 9345.59\n",
            "Num timesteps: 3100000\n",
            "Best mean reward: 9418.82 - Last mean reward per episode: 9323.18\n",
            "Num timesteps: 3150000\n",
            "Best mean reward: 9418.82 - Last mean reward per episode: 9322.76\n",
            "Num timesteps: 3200000\n",
            "Best mean reward: 9418.82 - Last mean reward per episode: 9339.12\n",
            "Num timesteps: 3250000\n",
            "Best mean reward: 9418.82 - Last mean reward per episode: 9358.24\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 5e+04       |\n",
            "|    ep_rew_mean          | 9.68e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 749         |\n",
            "|    iterations           | 1600        |\n",
            "|    time_elapsed         | 4374        |\n",
            "|    total_timesteps      | 3276800     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008611424 |\n",
            "|    clip_fraction        | 0.0498      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.486      |\n",
            "|    explained_variance   | -37.9       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.77        |\n",
            "|    n_updates            | 22570       |\n",
            "|    policy_gradient_loss | -0.0027     |\n",
            "|    value_loss           | 13.5        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 3300000\n",
            "Best mean reward: 9418.82 - Last mean reward per episode: 9350.13\n",
            "Num timesteps: 3350000\n",
            "Best mean reward: 9418.82 - Last mean reward per episode: 9354.13\n",
            "Num timesteps: 3400000\n",
            "Best mean reward: 9418.82 - Last mean reward per episode: 9358.03\n",
            "Num timesteps: 3450000\n",
            "Best mean reward: 9418.82 - Last mean reward per episode: 9382.86\n",
            "Num timesteps: 3500000\n",
            "Best mean reward: 9418.82 - Last mean reward per episode: 9384.01\n",
            "Num timesteps: 3550000\n",
            "Best mean reward: 9418.82 - Last mean reward per episode: 9392.65\n",
            "Num timesteps: 3600000\n",
            "Best mean reward: 9418.82 - Last mean reward per episode: 9392.21\n",
            "Num timesteps: 3650000\n",
            "Best mean reward: 9418.82 - Last mean reward per episode: 9392.95\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 5e+04       |\n",
            "|    ep_rew_mean          | 9.69e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 749         |\n",
            "|    iterations           | 1800        |\n",
            "|    time_elapsed         | 4920        |\n",
            "|    total_timesteps      | 3686400     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010038375 |\n",
            "|    clip_fraction        | 0.051       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.214      |\n",
            "|    explained_variance   | -2.8        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.33        |\n",
            "|    n_updates            | 24570       |\n",
            "|    policy_gradient_loss | -0.00671    |\n",
            "|    value_loss           | 3.46        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 3700000\n",
            "Best mean reward: 9418.82 - Last mean reward per episode: 9406.27\n",
            "Num timesteps: 3750000\n",
            "Best mean reward: 9418.82 - Last mean reward per episode: 9735.94\n",
            "Saving new best model to /log/IOE_combine4/RBF/best_model.zip\n",
            "Num timesteps: 3800000\n",
            "Best mean reward: 9735.94 - Last mean reward per episode: 9795.63\n",
            "Saving new best model to /log/IOE_combine4/RBF/best_model.zip\n",
            "Num timesteps: 3850000\n",
            "Best mean reward: 9795.63 - Last mean reward per episode: 9827.98\n",
            "Saving new best model to /log/IOE_combine4/RBF/best_model.zip\n",
            "Num timesteps: 3900000\n",
            "Best mean reward: 9827.98 - Last mean reward per episode: 9849.11\n",
            "Saving new best model to /log/IOE_combine4/RBF/best_model.zip\n",
            "Num timesteps: 3950000\n",
            "Best mean reward: 9849.11 - Last mean reward per episode: 9854.76\n",
            "Saving new best model to /log/IOE_combine4/RBF/best_model.zip\n",
            "Num timesteps: 4000000\n",
            "Best mean reward: 9854.76 - Last mean reward per episode: 9859.03\n",
            "Saving new best model to /log/IOE_combine4/RBF/best_model.zip\n",
            "Num timesteps: 4050000\n",
            "Best mean reward: 9859.03 - Last mean reward per episode: 9874.58\n",
            "Saving new best model to /log/IOE_combine4/RBF/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 5e+04       |\n",
            "|    ep_rew_mean          | 9.78e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 749         |\n",
            "|    iterations           | 2000        |\n",
            "|    time_elapsed         | 5465        |\n",
            "|    total_timesteps      | 4096000     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004711229 |\n",
            "|    clip_fraction        | 0.0794      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.399      |\n",
            "|    explained_variance   | -79.1       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 15.9        |\n",
            "|    n_updates            | 26570       |\n",
            "|    policy_gradient_loss | -0.00379    |\n",
            "|    value_loss           | 31          |\n",
            "-----------------------------------------\n",
            "Num timesteps: 4100000\n",
            "Best mean reward: 9874.58 - Last mean reward per episode: 9872.67\n",
            "Num timesteps: 4150000\n",
            "Best mean reward: 9874.58 - Last mean reward per episode: 9887.32\n",
            "Saving new best model to /log/IOE_combine4/RBF/best_model.zip\n",
            "Num timesteps: 4200000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9852.43\n",
            "Num timesteps: 4250000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9813.93\n",
            "Num timesteps: 4300000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9799.46\n",
            "Num timesteps: 4350000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9770.70\n",
            "Num timesteps: 4400000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9744.12\n",
            "Num timesteps: 4450000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9737.50\n",
            "Num timesteps: 4500000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9737.79\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 5e+04      |\n",
            "|    ep_rew_mean          | 9.68e+03   |\n",
            "| time/                   |            |\n",
            "|    fps                  | 749        |\n",
            "|    iterations           | 2200       |\n",
            "|    time_elapsed         | 6011       |\n",
            "|    total_timesteps      | 4505600    |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00817037 |\n",
            "|    clip_fraction        | 0.0446     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.552     |\n",
            "|    explained_variance   | -16.4      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 13.8       |\n",
            "|    n_updates            | 28570      |\n",
            "|    policy_gradient_loss | -0.00132   |\n",
            "|    value_loss           | 28.7       |\n",
            "----------------------------------------\n",
            "Num timesteps: 4550000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9735.01\n",
            "Num timesteps: 4600000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9739.02\n",
            "Num timesteps: 4650000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9758.15\n",
            "Num timesteps: 4700000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9757.11\n",
            "Num timesteps: 4750000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9767.24\n",
            "Num timesteps: 4800000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9777.80\n",
            "Num timesteps: 4850000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9788.77\n",
            "Num timesteps: 4900000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9795.12\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 5e+04        |\n",
            "|    ep_rew_mean          | 9.8e+03      |\n",
            "| time/                   |              |\n",
            "|    fps                  | 749          |\n",
            "|    iterations           | 2400         |\n",
            "|    time_elapsed         | 6555         |\n",
            "|    total_timesteps      | 4915200      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0077801105 |\n",
            "|    clip_fraction        | 0.0456       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.267       |\n",
            "|    explained_variance   | -4.47        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 10.4         |\n",
            "|    n_updates            | 30570        |\n",
            "|    policy_gradient_loss | -0.00525     |\n",
            "|    value_loss           | 27.1         |\n",
            "------------------------------------------\n",
            "Num timesteps: 4950000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9796.16\n",
            "Num timesteps: 5000000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9823.47\n",
            "Num timesteps: 5050000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9830.16\n",
            "Num timesteps: 5100000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9824.83\n",
            "Num timesteps: 5150000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9837.37\n",
            "Num timesteps: 5200000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9831.31\n",
            "Num timesteps: 5250000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9834.07\n",
            "Num timesteps: 5300000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9855.50\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 5e+04       |\n",
            "|    ep_rew_mean          | 9.86e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 750         |\n",
            "|    iterations           | 2600        |\n",
            "|    time_elapsed         | 7099        |\n",
            "|    total_timesteps      | 5324800     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018065587 |\n",
            "|    clip_fraction        | 0.0902      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.785      |\n",
            "|    explained_variance   | -19.4       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.43        |\n",
            "|    n_updates            | 32570       |\n",
            "|    policy_gradient_loss | -0.00352    |\n",
            "|    value_loss           | 15.8        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 5350000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9843.28\n",
            "Num timesteps: 5400000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9840.91\n",
            "Num timesteps: 5450000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9836.65\n",
            "Num timesteps: 5500000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9851.79\n",
            "Num timesteps: 5550000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9832.79\n",
            "Num timesteps: 5600000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9840.73\n",
            "Num timesteps: 5650000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9828.22\n",
            "Num timesteps: 5700000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9833.05\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 5e+04         |\n",
            "|    ep_rew_mean          | 9.83e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 750           |\n",
            "|    iterations           | 2800          |\n",
            "|    time_elapsed         | 7642          |\n",
            "|    total_timesteps      | 5734400       |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00047441182 |\n",
            "|    clip_fraction        | 0.0259        |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.224        |\n",
            "|    explained_variance   | -18.4         |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 4.28          |\n",
            "|    n_updates            | 34570         |\n",
            "|    policy_gradient_loss | -0.00351      |\n",
            "|    value_loss           | 10.9          |\n",
            "-------------------------------------------\n",
            "Num timesteps: 5750000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9818.25\n",
            "Num timesteps: 5800000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9836.22\n",
            "Num timesteps: 5850000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9841.68\n",
            "Num timesteps: 5900000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9823.27\n",
            "Num timesteps: 5950000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9827.16\n",
            "Num timesteps: 6000000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9824.30\n",
            "Num timesteps: 6050000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9834.40\n",
            "Num timesteps: 6100000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9833.57\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 5e+04        |\n",
            "|    ep_rew_mean          | 9.83e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 750          |\n",
            "|    iterations           | 3000         |\n",
            "|    time_elapsed         | 8185         |\n",
            "|    total_timesteps      | 6144000      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0054090754 |\n",
            "|    clip_fraction        | 0.0501       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.336       |\n",
            "|    explained_variance   | -97.7        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 21.2         |\n",
            "|    n_updates            | 36570        |\n",
            "|    policy_gradient_loss | -0.00322     |\n",
            "|    value_loss           | 61           |\n",
            "------------------------------------------\n",
            "Num timesteps: 6150000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9839.95\n",
            "Num timesteps: 6200000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9828.42\n",
            "Num timesteps: 6250000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9817.65\n",
            "Num timesteps: 6300000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9787.62\n",
            "Num timesteps: 6350000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9760.73\n",
            "Num timesteps: 6400000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9750.48\n",
            "Num timesteps: 6450000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9797.26\n",
            "Num timesteps: 6500000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9799.98\n",
            "Num timesteps: 6550000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9769.93\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 5e+04       |\n",
            "|    ep_rew_mean          | 9.77e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 750         |\n",
            "|    iterations           | 3200        |\n",
            "|    time_elapsed         | 8732        |\n",
            "|    total_timesteps      | 6553600     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011110001 |\n",
            "|    clip_fraction        | 0.0646      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.535      |\n",
            "|    explained_variance   | -0.63       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 15.5        |\n",
            "|    n_updates            | 38570       |\n",
            "|    policy_gradient_loss | -0.00529    |\n",
            "|    value_loss           | 24.3        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 6600000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9797.30\n",
            "Num timesteps: 6650000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9778.81\n",
            "Num timesteps: 6700000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9780.83\n",
            "Num timesteps: 6750000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9766.65\n",
            "Num timesteps: 6800000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9730.22\n",
            "Num timesteps: 6850000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9696.60\n",
            "Num timesteps: 6900000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9689.68\n",
            "Num timesteps: 6950000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9674.42\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 5e+04        |\n",
            "|    ep_rew_mean          | 9.67e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 750          |\n",
            "|    iterations           | 3400         |\n",
            "|    time_elapsed         | 9277         |\n",
            "|    total_timesteps      | 6963200      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0036451302 |\n",
            "|    clip_fraction        | 0.0486       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.465       |\n",
            "|    explained_variance   | -106         |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 17.6         |\n",
            "|    n_updates            | 40570        |\n",
            "|    policy_gradient_loss | -0.00185     |\n",
            "|    value_loss           | 41.2         |\n",
            "------------------------------------------\n",
            "Num timesteps: 7000000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9666.03\n",
            "Num timesteps: 7050000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9639.30\n",
            "Num timesteps: 7100000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9622.90\n",
            "Num timesteps: 7150000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9622.65\n",
            "Num timesteps: 7200000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9623.60\n",
            "Num timesteps: 7250000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9628.73\n",
            "Num timesteps: 7300000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9657.69\n",
            "Num timesteps: 7350000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9642.39\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 5e+04        |\n",
            "|    ep_rew_mean          | 9.64e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 750          |\n",
            "|    iterations           | 3600         |\n",
            "|    time_elapsed         | 9824         |\n",
            "|    total_timesteps      | 7372800      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0032032588 |\n",
            "|    clip_fraction        | 0.048        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.776       |\n",
            "|    explained_variance   | -0.0418      |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.37         |\n",
            "|    n_updates            | 42570        |\n",
            "|    policy_gradient_loss | -0.00453     |\n",
            "|    value_loss           | 31.4         |\n",
            "------------------------------------------\n",
            "Num timesteps: 7400000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9623.51\n",
            "Num timesteps: 7450000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9605.18\n",
            "Num timesteps: 7500000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9608.75\n",
            "Num timesteps: 7550000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9614.18\n",
            "Num timesteps: 7600000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9621.64\n",
            "Num timesteps: 7650000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9622.85\n",
            "Num timesteps: 7700000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9634.17\n",
            "Num timesteps: 7750000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9635.10\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 5e+04        |\n",
            "|    ep_rew_mean          | 9.64e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 750          |\n",
            "|    iterations           | 3800         |\n",
            "|    time_elapsed         | 10370        |\n",
            "|    total_timesteps      | 7782400      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0044412958 |\n",
            "|    clip_fraction        | 0.0296       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.311       |\n",
            "|    explained_variance   | -39.6        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 8.31         |\n",
            "|    n_updates            | 44570        |\n",
            "|    policy_gradient_loss | -0.00548     |\n",
            "|    value_loss           | 16.9         |\n",
            "------------------------------------------\n",
            "Num timesteps: 7800000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9628.57\n",
            "Num timesteps: 7850000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9627.06\n",
            "Num timesteps: 7900000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9643.46\n",
            "Num timesteps: 7950000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9690.35\n",
            "Num timesteps: 8000000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9716.45\n",
            "Num timesteps: 8050000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9733.46\n",
            "Num timesteps: 8100000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9766.61\n",
            "Num timesteps: 8150000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9774.67\n",
            "---------------------------------------\n",
            "| rollout/                |           |\n",
            "|    ep_len_mean          | 5e+04     |\n",
            "|    ep_rew_mean          | 9.77e+03  |\n",
            "| time/                   |           |\n",
            "|    fps                  | 750       |\n",
            "|    iterations           | 4000      |\n",
            "|    time_elapsed         | 10916     |\n",
            "|    total_timesteps      | 8192000   |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0082424 |\n",
            "|    clip_fraction        | 0.0667    |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -0.678    |\n",
            "|    explained_variance   | -7.87     |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 50.3      |\n",
            "|    n_updates            | 46570     |\n",
            "|    policy_gradient_loss | -0.00499  |\n",
            "|    value_loss           | 134       |\n",
            "---------------------------------------\n",
            "Num timesteps: 8200000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9702.67\n",
            "Num timesteps: 8250000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9658.31\n",
            "Num timesteps: 8300000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9643.17\n",
            "Num timesteps: 8350000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9638.73\n",
            "Num timesteps: 8400000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9635.29\n",
            "Num timesteps: 8450000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9609.71\n",
            "Num timesteps: 8500000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9603.56\n",
            "Num timesteps: 8550000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9577.17\n",
            "Num timesteps: 8600000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9572.02\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 5e+04        |\n",
            "|    ep_rew_mean          | 9.57e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 750          |\n",
            "|    iterations           | 4200         |\n",
            "|    time_elapsed         | 11462        |\n",
            "|    total_timesteps      | 8601600      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0053464225 |\n",
            "|    clip_fraction        | 0.0492       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.38        |\n",
            "|    explained_variance   | -72.6        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 7.17         |\n",
            "|    n_updates            | 48570        |\n",
            "|    policy_gradient_loss | -0.0017      |\n",
            "|    value_loss           | 15.6         |\n",
            "------------------------------------------\n",
            "Num timesteps: 8650000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9558.31\n",
            "Num timesteps: 8700000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9547.53\n",
            "Num timesteps: 8750000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9575.12\n",
            "Num timesteps: 8800000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9579.38\n",
            "Num timesteps: 8850000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9570.44\n",
            "Num timesteps: 8900000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9529.57\n",
            "Num timesteps: 8950000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9516.88\n",
            "Num timesteps: 9000000\n",
            "Best mean reward: 9887.32 - Last mean reward per episode: 9515.91\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<stable_baselines3.ppo.ppo.PPO at 0x7f6661e0f090>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xswZyihJ3InE",
        "outputId": "a2bde9bd-73f2-44ba-9ae7-49a21798cd60"
      },
      "source": [
        "env.number_of_calls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "183"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "wpJOixyTZ0KM",
        "outputId": "6a56a3dd-b25a-40b1-dc98-ed904cc4238e"
      },
      "source": [
        "plot_results(log_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dO4eaX6VTA86"
      },
      "source": [
        "model.save(\"models/saved_model\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}